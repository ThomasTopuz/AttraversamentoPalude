{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZEco2HK6D57"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3nCUqopXHwv"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 0x0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjTkUw7BWulH"
      },
      "source": [
        "# Lab 03: Gradient Descent and Polynomial Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNnZUk36Xz7_"
      },
      "source": [
        "For the first task, we will work with synthetic univariate data. This is the same data as in the previous Lab.\n",
        "We generate $100$ features $x_i \\in [-1, 1]$ as `x` and a regression target `y1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ojta777H2ulb"
      },
      "outputs": [],
      "source": [
        "data_rng = np.random.default_rng(RANDOM_SEED)\n",
        "n = 100\n",
        "x = 2 * data_rng.random(n) - 1  # create n points between -1 and 1\n",
        "\n",
        "# setup synthetic linear data\n",
        "true_offset = 0.5\n",
        "true_slope = 1.25\n",
        "noise = data_rng.normal(loc=0., scale=0.25, size=(n,))\n",
        "\n",
        "y1 = true_offset + true_slope * x + noise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what the data looks like:"
      ],
      "metadata": {
        "id": "_ww1rAls1AN-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxYMdhfxyYAd"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y1)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xv-GgvWGxfql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL31gChVqLpC"
      },
      "source": [
        "### 游닉 Task 1A (3 points)游닉\n",
        "\n",
        "In class you have seen that, instead of using the closed form solutions, we can also estimate the parameters $\\theta_i$ of the linear regression models by using Gradient Descent.\n",
        "\n",
        "For the univariate linear regression model, the stochastic gradient descent updates (from step t to t+1) look like this:\n",
        "* $\\theta_{0}^{(t+1)} = \\theta_{0}^{(t)} - \\alpha (\\theta_{0}^{(t)} + \\theta_{1}^{(t)} x_t - y_t)$\n",
        "* $\\theta_{1}^{(t+1)} = \\theta_{1}^{(t)} - \\alpha (\\theta_{0}^{(t)} + \\theta_{1}^{(t)} x_t - y_t) x_t$\n",
        "\n",
        "Here $\\alpha$ is the learning rate, and $(x_t, y_t)$ is the data point sampled\n",
        "at time $t$.\n",
        "\n",
        "\n",
        "In the following cell, implement the `.fit` and `.predict` methods:\n",
        "* In the `.predict` method, apply the model to the input `x`.\n",
        "* In the `.fit` method, implement the update equations for\n",
        "$\\theta_0$ and $\\theta_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJMHvQmXmVKr"
      },
      "outputs": [],
      "source": [
        "class SGDUnivariateLinearRegression:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.theta_0: float = 0.\n",
        "    self.theta_1: float = 0.\n",
        "    self.rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "  def predict(self, x):\n",
        "    y = self.theta_0 + self.theta_1 * x\n",
        "    return y  # TODO\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self, x, y, n_iter: int = 100, learning_rate: float = 1.0):\n",
        "    for t in range(n_iter):\n",
        "      sample_ix = self.rng.integers(0, len(x))\n",
        "\n",
        "      xt = x[sample_ix]\n",
        "      yt = y[sample_ix]\n",
        "\n",
        "      t0 = self.theta_0\n",
        "      t1 = self.theta_1\n",
        "\n",
        "      self.theta_0 = t0 - learning_rate * (t0 + t1 *xt - yt)\n",
        "      self.theta_1 = t1 - learning_rate * (t0 + t1 *xt - yt) * xt\n",
        "      # TODO: update self.theta_0 and self.theta_1 SIMULTANEOUSLY (!!!) according to their update equations\n",
        "\n",
        "    return self"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "游닉 **<mark>On Moodle</mark>** 游닉\n",
        "\n",
        "* Hand in your code from the cell above."
      ],
      "metadata": {
        "id": "i_zwIMN3INHh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHLBmTm4vK9p"
      },
      "source": [
        "### 游닉 Task 1B (2 points)游닉\n",
        "\n",
        "Run SGD for `x` and the target `y1` and compute the mean squared error (MSE).\n",
        "The MSE is defined as: $\\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$, where\n",
        "$\\hat{y}$ are the model predictions.\n",
        "\n",
        "* Create an instance of the class `SGDUnivariateLinearRegression`\n",
        "* fit the model using its `.fit` method\n",
        "* get the predicted values, using `.predict`\n",
        "* implement the `mse` function\n",
        "* compute the MSE of your predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ1szyQhK9so"
      },
      "outputs": [],
      "source": [
        "def mse(y_pred, y_true):\n",
        "  #TODO\n",
        "  sum = 0\n",
        "  for i in range(len(y_pred)):\n",
        "    sum += (y_pred[i] - y_true[i])**2\n",
        "\n",
        "  return sum / len(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V35vBU5Yti8Z"
      },
      "outputs": [],
      "source": [
        "# TODO: implement fit an predict\n",
        "model = SGDUnivariateLinearRegression()\n",
        "\n",
        "# model.fit(x, y1, n_iter=20000, learning_rate=0.001)\n",
        "model.fit(x, y1)\n",
        "y_pred = model.predict(x)\n",
        "error = mse(y_pred, y1)\n",
        "print(\"MSE: \", error)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "游닉 **<mark>On Moodle</mark>** 游닉\n",
        "* Report the resulting Mean Squared Error"
      ],
      "metadata": {
        "id": "n_j0ybGFIoBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to plot the resulting model, you can use the following function:"
      ],
      "metadata": {
        "id": "60862LLY2ZGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0eKDuRt1YOF"
      },
      "outputs": [],
      "source": [
        "def plot_model(x, y_pred, y_true, title):\n",
        "  plt.scatter(x, y_true)\n",
        "  plt.plot(x, y_pred, c=\"r\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "plot_model(x, y_pred, y1, \"Model of Gradient Descent for y1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSsE1o6GwA3K"
      },
      "source": [
        "### 游닉 Task 1C (4 points) 游닉\n",
        "\n",
        "You will now plot the learning curves of gradient descent for different learning rates $\\alpha$.\n",
        "A learning curves shows how a model's performance changes with increasing number of update steps.\n",
        "In our case we will plot the model's MSE as a function of the number of update\n",
        "steps `n_iter` for different values of `learning_rate`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For comparison, we first implement the closed form for univariate regression from previous lab:"
      ],
      "metadata": {
        "id": "x6nqOoOpzmMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnivariateLinearRegression:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.theta_0: float = 0.\n",
        "    self.theta_1: float = 0.\n",
        "\n",
        "  def predict(self, x):\n",
        "    return self.theta_0 + self.theta_1 * x\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    mean_x = np.mean(x)\n",
        "    mean_y = np.mean(y)\n",
        "\n",
        "    cent_x = x - mean_x\n",
        "    cent_y = y - mean_y\n",
        "\n",
        "    theta_1 = np.sum(cent_x * cent_y) / np.sum(cent_x * cent_x)\n",
        "    theta_0 = mean_y - theta_1 * mean_x\n",
        "\n",
        "    self.theta_1 = theta_1\n",
        "    self.theta_0 = theta_0\n",
        "\n",
        "    return self"
      ],
      "metadata": {
        "id": "gjK-QunEzkZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closed_form = UnivariateLinearRegression()\n",
        "y_pred = closed_form.fit(x, y1).predict(x)\n",
        "\n",
        "\n",
        "plot_model(x, y_pred, y1, \"Model of Gradient Descent for y1\")"
      ],
      "metadata": {
        "id": "M33V17233Ur7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the following cell we setup most of the scaffold to create learning curves. A learning curve contains the MSE for different numbers of iterations, i.e. different number of updates of the parameters in 풪.\n",
        "\n",
        "We will draw learning curves for different values of learning rate 풤 (don't get confused with the duplicate usage of \"learning\" here. The first refers to the improvement of MSE over number of iterations, while the second influences how much we change values in 풪 in each step).\n",
        "\n",
        "Follow the instructions in the comments to finish the plots."
      ],
      "metadata": {
        "id": "j6W3hr76JKvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rr5ix7LNISB"
      },
      "outputs": [],
      "source": [
        "n_iters = [50, 100, 200, 500, 1000, 2000]\n",
        "learning_rates = [1., .1, .01]\n",
        "\n",
        "# we plot the MSE achieved by the closed form model as a reference\n",
        "closed_form = UnivariateLinearRegression()\n",
        "closed_form.fit(x, y1)\n",
        "mse_base = mse(y_pred=closed_form.predict(x), y_true=y1)\n",
        "plt.plot(n_iters, np.ones_like(n_iters) * mse_base, label=\"closed form\", linestyle='--', c='b')\n",
        "\n",
        "model = SGDUnivariateLinearRegression()\n",
        "for alpha in learning_rates:\n",
        "  mses = []\n",
        "  for n_iter in n_iters:\n",
        "    # fit a SGDUnivariateLinearRegression model using n_iter=n_iter and\n",
        "    # learning_rate=alpha\n",
        "    # compute its mse and append the mse value to the mses list\n",
        "\n",
        "\n",
        "    model.fit(x, y1, n_iter=n_iter, learning_rate=alpha)\n",
        "    y_pred = model.predict(x)\n",
        "    mse_ = mse(y_pred, y1)  # replace with mse calculation\n",
        "    mses.append(mse_)\n",
        "  plt.plot(n_iters, mses, label=f\"alpha = {alpha:.2f}\")\n",
        "\n",
        "plt.xlabel(\"n_iter\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmCkMMJyEEgV"
      },
      "source": [
        "游닉 **<mark>On Moodle</mark>** 游닉\n",
        "\n",
        "* The final plot containing the 3 learning curves\n",
        "* A short (2-3 sentences) interpretation of the curves: why do you think they look the way they do? Can you draw any conclusions?\n",
        "\n",
        "In case you were not able to arrive at the final plot:\n",
        "\n",
        "* include screenshots of the code you wrote so we can assign partial credit\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia9s_Q-KXf0T"
      },
      "source": [
        "# TASK 2: Polynomial Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV0Z3OdeXpha"
      },
      "source": [
        "We now switch to polynomial regression. First, let's create and explore some new synthetic data for this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb5WsezldFla"
      },
      "outputs": [],
      "source": [
        "# set the random seed to an integer, so that everyone has the same data to work with\n",
        "np.random.seed(seed=RANDOM_SEED)\n",
        "# create predictor variable that has a standard normal distribution, and reshape it in order to use it for the model training\n",
        "x = np.random.normal(0, 1, 100).reshape(-1, 1)\n",
        "# create target variable\n",
        "y = 3*x**3 + 2*x**2 + x + np.random.normal(0, 10, 100).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E65IxT1Bwpmk"
      },
      "source": [
        "Visualise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCZgTYP3fZe7"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx8aSpUnJCI7"
      },
      "source": [
        "First, we try to fit a *Linear* Regression model to the data.\n",
        "\n",
        "For this, we first split the data into a train set and a test set (80% train, 20% test data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti7myWk7KS8Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFSaakYLJuK7"
      },
      "source": [
        "## Task 2A (1 point)\n",
        "1. Apply Linear Regression on the data and predict y values for training set as well for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez6t4Q4P82Qo"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "# TODO\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1k6VBGk8yI6"
      },
      "source": [
        "2. Calculate MSE for training as well as for test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJGjXK8aKD8q"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# TODO\n",
        "mse_train = mean_squared_error(y_train, y_train_predict)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "print(f\"MSE of training data: {mse_train}\")\n",
        "print(f\"MSE of test data: {mse_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VOmhngKEQL"
      },
      "source": [
        "3. Visualize the model. Plot all the data points als well as the regression model when applied to the training set and to the test set (in plt.scatter, you can set the color of points e.g. to red using \" color='r' \")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLwMEWirLLBA"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Scatter plots of data\n",
        "plt.scatter(X_train, y_train, color=\"r\", label=\"Train data\")\n",
        "plt.scatter(X_test, y_test, color=\"b\", label=\"Test data\")\n",
        "\n",
        "# To plot regression lines, we sort X for a clean line\n",
        "X_line = np.linspace(min(X_train.min(), X_test.min()),\n",
        "                     max(X_train.max(), X_test.max()), 100).reshape(-1, 1)\n",
        "\n",
        "y_line = model.predict(X_line)\n",
        "\n",
        "# Regression line\n",
        "plt.plot(X_line, y_line, color=\"k\", label=\"Regression line\")\n",
        "\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.title(\"Linear Regression: Train & Test Visualization\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "游닉 **<mark>On Moodle</mark>** 游닉\n",
        "\n",
        "Upload\n",
        "* your scatter plot"
      ],
      "metadata": {
        "id": "tC6BqZC5Pcja"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwsmpB3oMJZf"
      },
      "source": [
        "## Task 2B (2 points)\n",
        "\n",
        "You will now investigate how well polynomial regression with a polynomial of degree 2 can solve the task.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that for polynomial regression, we introduce artificial variables $z_i$ and a new hypothesis $h(z) = \\theta_0 z_0 + \\theta_1 z_1 + \\theta_2 z_2 + \\theta_3 z_3 ...$, where each $z_i$ is a polynomial in $x$. One of the simplest hypothesis is quadratic, where we have $h(z) = \\theta_0 z_0 + \\theta_1 z_1 + \\theta_2 z_2$ with $z_0 = 1, z_1 = x$ and $z_2 = x^2$.\n",
        "\n",
        "In the following cell, implement polynomial regression with the quadratic hypothesis above, and plot the result."
      ],
      "metadata": {
        "id": "Mqj9E-PQErR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train_poly, y_train)\n",
        "\n",
        "y_train_pred = poly_model.predict(X_train_poly)\n",
        "y_test_pred = poly_model.predict(X_test_poly)\n",
        "\n",
        "mse_train_poly = mean_squared_error(y_train, y_train_pred)\n",
        "mse_test_poly = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "X_line = np.linspace(min(X_train.min(), X_test.min()),\n",
        "                     max(X_train.max(), X_test.max()), 200).reshape(-1, 1)\n",
        "X_line_poly = poly.transform(X_line)\n",
        "y_line = poly_model.predict(X_line_poly)"
      ],
      "metadata": {
        "id": "dBRl5Yu5F5Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the result\n"
      ],
      "metadata": {
        "id": "GF5b2e6aJmfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "plt.scatter(X_train, y_train, color=\"r\", label=\"Train data\")\n",
        "plt.scatter(X_test, y_test, color=\"b\", label=\"Test data\")\n",
        "plt.plot(X_line, y_line, color=\"k\", label=\"Polynomial degree 2 fit\")\n",
        "\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.title(\"Polynomial Regression (degree=2)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9aZe6xHFJphT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of implementing the polynomial transformation by hand, we can use [PolynomialFeatures.fit_transform](https://scikit-learn.org/stable/modules/preprocessing.html#polynomial-features) from sklearn, which generates the new features automatically.\n",
        "\n",
        "1. Transform the data accordingly to describe polynomial distribution of degree=2\n",
        "2. Train a Linear Regression model on polynomial data\n",
        "3. Make predictions for training data\n",
        "4. Make predictions for test data\n",
        "5. Calculate MSE for training as well as test data"
      ],
      "metadata": {
        "id": "zZjSeX5EE6Oh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxni0o041MYH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# TODO\n",
        "\n",
        "print(f\"MSE of training data: {mse_train_poly}\")\n",
        "print(f\"MSE of test data: {mse_test_poly}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhbIv-toOFoV"
      },
      "source": [
        "Did it perform better than Linear Regression? Visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFNrIwDuOUXo"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Fit linear model\n",
        "lin = LinearRegression().fit(X_train, y_train)\n",
        "ytr_lin = lin.predict(X_train)\n",
        "yte_lin = lin.predict(X_test)\n",
        "\n",
        "# Fit degree-2 polynomial model\n",
        "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "Xtr2 = poly.fit_transform(X_train)\n",
        "Xte2 = poly.transform(X_test)\n",
        "poly2 = LinearRegression().fit(Xtr2, y_train)\n",
        "ytr_poly2 = poly2.predict(Xtr2)\n",
        "yte_poly2 = poly2.predict(Xte2)\n",
        "\n",
        "# MSEs\n",
        "lin_tr_mse = mean_squared_error(y_train, ytr_lin)\n",
        "lin_te_mse = mean_squared_error(y_test, yte_lin)\n",
        "poly2_tr_mse = mean_squared_error(y_train, ytr_poly2)\n",
        "poly2_te_mse = mean_squared_error(y_test, yte_poly2)\n",
        "\n",
        "print(f\"Linear -> Train MSE: {lin_tr_mse:.4f} | Test MSE: {lin_te_mse:.4f}\")\n",
        "print(f\"Poly d=2 -> Train MSE: {poly2_tr_mse:.4f} | Test MSE: {poly2_te_mse:.4f}\")\n",
        "print(\"Degree-2 better on TEST? \", \"YES\" if poly2_te_mse < lin_te_mse else \"NO\")\n",
        "\n",
        "# Visualization\n",
        "X_line = np.linspace(min(X_train.min(), X_test.min()),\n",
        "                     max(X_train.max(), X_test.max()), 400).reshape(-1, 1)\n",
        "y_line_lin = lin.predict(X_line)\n",
        "y_line_poly2 = poly2.predict(poly.transform(X_line))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(X_train, y_train, color='r', alpha=0.6, label='Train')\n",
        "plt.scatter(X_test, y_test, color='b', alpha=0.6, label='Test')\n",
        "plt.plot(X_line, y_line_lin, label='Linear fit', linewidth=2)\n",
        "plt.plot(X_line, y_line_poly2, label='Polynomial fit (degree=2)', linewidth=2)\n",
        "plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.title(\"Linear vs Degree-2 Polynomial Regression\")\n",
        "plt.legend(); plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "游닉 **<mark>On Moodle</mark>** 游닉\n",
        "\n",
        "Upload your scatter plot\n"
      ],
      "metadata": {
        "id": "2sYVU9lQPxXD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR_v9mTWOVNj"
      },
      "source": [
        "## Task 2C (4 point)\n",
        "Finally, we investigate the influence of the polynomial degree on the results. Consider degrees in range(0, 11). Plot MSE (on training as well as test data) depending on the polynomial degree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YFTQqWZO_Jx"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QpfImYEI1l"
      },
      "source": [
        "游닉 **<mark>On Moodle</mark>** 游닉\n",
        "\n",
        "* Upload your plot of MSE for training and test data\n",
        "\n",
        "* Answer the following three questions in a text:\n",
        "1. What is the optimal value of the polynomial degrees?\n",
        "2. Do the values of MSE training and MSE test behave similarly?\n",
        "3. What happens in the models with polynomial degrees >= 8?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "degrees = range(0, 11)\n",
        "train_mses, test_mses = [], []\n",
        "\n",
        "for d in degrees:\n",
        "    poly = PolynomialFeatures(degree=d, include_bias=True)\n",
        "    Xtr = poly.fit_transform(X_train)\n",
        "    Xte = poly.transform(X_test)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(Xtr, y_train)\n",
        "\n",
        "    ytr_pred = model.predict(Xtr)\n",
        "    yte_pred = model.predict(Xte)\n",
        "\n",
        "    train_mses.append(mean_squared_error(y_train, ytr_pred))\n",
        "    test_mses.append(mean_squared_error(y_test, yte_pred))\n",
        "\n",
        "plt.plot(degrees, train_mses, marker='o', label='Train MSE')\n",
        "plt.plot(degrees, test_mses, marker='o', label='Test MSE')\n",
        "plt.xticks(degrees)\n",
        "plt.xlabel('Polynomial degree')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('Train/Test MSE vs Polynomial Degree')\n",
        "plt.show()\n",
        "\n",
        "best_deg = degrees[int(np.argmin(test_mses))]\n",
        "print(\"Best degree by test MSE:\", best_deg)\n"
      ],
      "metadata": {
        "id": "0BJrUzpkQc94"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}